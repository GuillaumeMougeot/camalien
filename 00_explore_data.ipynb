{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec13d5-55c7-4c64-8243-3b57ed559ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from time import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import cv2\n",
    "# import multiprocessing\n",
    "# multiprocessing.set_start_method(\"spawn\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c34fc5-14b8-4b83-a69e-b3f67e80cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_pth = Path(r\"C:\\Users\\au761367\\Datasets\\classif\\camalien\\camalien_images_and_responsefiles_2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2dc93b-d65f-4e06-900e-4bf6252fb10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(csv_pth, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60edadf7-b17d-4dd5-abc2-ff7fac50fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8182722-52ce-4bb7-a238-dec0bcc51587",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['country','timestamp']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e1a0b-d70e-46cb-88ab-3a5a646ae274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('country').agg('count').path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e6431-3ccc-4873-a1b1-7d77f1bae9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62302525-6fa0-43d4-bbab-8dec6ddc5b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format=\"ISO8601\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f8e206-fc0b-4014-96d9-3e1c39e0baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = df['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1dab93-a54e-4c66-a215-fb5d9fd4ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_dist(df):\n",
    "    df=df.copy()\n",
    "    for country in countries:\n",
    "        subset = df[df['country'] == country]\n",
    "    \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(subset['timestamp'], bins=30)   # adjust bins as needed\n",
    "        plt.title(f\"Timestamp Histogram – {country}\")\n",
    "        plt.xlabel(\"Timestamp\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        break\n",
    "# plot_dist(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4f1c1-fbe9-420f-8871-10a4045c90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fda202-5989-4904-bd62-f2450e50124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"2024-06-11\"\n",
    "\n",
    "start = pd.to_datetime(date, utc=True).normalize()\n",
    "end   = start + pd.Timedelta(days=1)\n",
    "\n",
    "filtered = df[(df['timestamp'] >= start) & (df['timestamp'] < end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3f8c9-909c-42f3-99ca-398c4f911b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_days = df['timestamp'].dt.date.unique()\n",
    "print(sorted(unique_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff7c06-afd4-458f-9566-aebba45ca7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] = df['timestamp'].dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51f694-076f-4975-9230-534f0a67128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['country', 'day', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dc7094-5830-4185-b542-8ac3a00ce337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['imageurl'].tail().iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8b59b-3dae-45a7-941f-5ca6c9f7d0c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ac724-3a26-47ab-a925-26f1884d9dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15\n",
    "\n",
    "sampled = (\n",
    "    df\n",
    "    .groupby(['country', 'day'])\n",
    "    .apply(lambda g: g.iloc[::N])   # take every N-th row\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1663a6-7a78-44ab-9dc5-ce621d7ea2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1130bf1-7348-47c6-9fcf-4993670b5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1205b-dc33-4210-bc1f-bffa90958b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_image(session, url, semaphore):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            async with session.get(url) as resp:\n",
    "                resp.raise_for_status()\n",
    "                data = await resp.read()\n",
    "                return Image.open(BytesIO(data))\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "async def fetch_many_images(urls, max_concurrency=16, timeout_seconds=10):\n",
    "    timeout = aiohttp.ClientTimeout(total=timeout_seconds)\n",
    "    connector = aiohttp.TCPConnector(limit=max_concurrency)\n",
    "\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "    async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:\n",
    "        tasks = [\n",
    "            fetch_image(session, url, semaphore)\n",
    "            for url in urls\n",
    "        ]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "def plot_sample_grid(df, start_idx=0, grid_size=4, url_col=\"imageurl\"):\n",
    "    \"\"\"\n",
    "    Plot a 4x4 grid (16 images) from df starting at index start_idx.\n",
    "    Downloads using the `imageurl` column.\n",
    "    \"\"\"\n",
    "\n",
    "    n_images = grid_size * grid_size\n",
    "    urls = df[url_col].iloc[start_idx : start_idx + n_images]\n",
    "\n",
    "    # run async loader\n",
    "    t1=time()\n",
    "    images = asyncio.run(fetch_many_images(urls, max_concurrency=16))\n",
    "    t2=time()\n",
    "\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(16, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, img in zip(axes, images):\n",
    "        if img is None:\n",
    "            ax.text(0.5, 0.5, \"Load error\", ha=\"center\", va=\"center\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    t3=time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bdc79b-0819-46bd-a455-597b7e1994ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_grid(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48813c-488e-4f1d-8bca-304e6d6e9f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['imageurl'].head(11).iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de1aec-4b6a-4b86-8e42-f504a6a09c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9649e781-a4a2-4819-b353-788ff887d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_and_save_image(session, url, save_path, semaphore):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            async with session.get(url) as resp:\n",
    "                resp.raise_for_status()\n",
    "                data = await resp.read()\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download image {url}: {e}\")\n",
    "\n",
    "async def download_and_compress_image(session, url, save_path, semaphore, target_width=1920, jpeg_quality=95):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            async with session.get(url) as resp:\n",
    "                resp.raise_for_status()\n",
    "                data = await resp.read()\n",
    "                # Load image\n",
    "                img = Image.open(BytesIO(data))\n",
    "                orig_size = img.size\n",
    "                # Resize proportionally\n",
    "                w, h = img.size\n",
    "                if w > target_width:\n",
    "                    new_h = int(target_width * h / w)\n",
    "                    img = img.resize((target_width, new_h), Image.LANCZOS)\n",
    "                # Save compressed\n",
    "                img.save(save_path, format=\"JPEG\", quality=jpeg_quality)\n",
    "                return orig_size, img.size\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download/compress {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "async def download_and_save_json(session, url, save_path, semaphore):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            async with session.get(url) as resp:\n",
    "                resp.raise_for_status()\n",
    "                data = await resp.text()\n",
    "                with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download JSON {url}: {e}\")\n",
    "\n",
    "async def save_images_and_json(df, img_dir:Path, json_dir:Path, max_concurrency:int=16):\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "    timeout = aiohttp.ClientTimeout(total=60)\n",
    "    connector = aiohttp.TCPConnector(limit=max_concurrency)\n",
    "\n",
    "    async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:\n",
    "        tasks = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            # Image\n",
    "            image_filename = f\"{row['imagedataid']}.jpg\"\n",
    "            image_path = img_dir / image_filename\n",
    "            tasks.append(download_and_save_image(session, row['imageurl'], image_path, semaphore))\n",
    "\n",
    "            # JSON\n",
    "            json_filename = f\"{row['imagedataid']}.json\"\n",
    "            json_path = json_dir / json_filename\n",
    "            tasks.append(download_and_save_json(session, row['pn_response'], json_path, semaphore))\n",
    "\n",
    "        await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9b38ca-db80-48a8-9073-6370566f8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(r\"D:\")\n",
    "img_dir = out_dir / \"images\"\n",
    "json_dir = out_dir / \"json\"\n",
    "img_dir.mkdir(exist_ok=True)\n",
    "json_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a3750-50da-4c36-a1f2-61bc64dd12b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asyncio.run(save_images_and_json(sampled, img_dir, json_dir, max_concurrency=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee3f64-f543-4008-aa59-97352e272c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compress_image_on_disk(input_path, output_path, target_width=1920, jpeg_quality=95):\n",
    "    \"\"\"\n",
    "    Compress an image on disk, resizing proportionally to target_width\n",
    "    and saving as JPEG with specified quality.\n",
    "    \n",
    "    Parameters:\n",
    "        input_path (str): Path to the original image\n",
    "        output_path (str): Path to save the compressed image\n",
    "        target_width (int): Maximum width of compressed image\n",
    "        jpeg_quality (int): JPEG quality (1-100)\n",
    "        \n",
    "    Returns:\n",
    "        orig_size (tuple): (width, height) of original image\n",
    "        compressed_size (tuple): (width, height) of compressed image\n",
    "    \"\"\"\n",
    "    img = Image.open(input_path)\n",
    "    orig_size = img.size\n",
    "\n",
    "    w, h = img.size\n",
    "    if w > target_width:\n",
    "        new_h = int(target_width * h / w)\n",
    "        img = img.resize((target_width, new_h), Image.LANCZOS)\n",
    "\n",
    "    img.save(output_path, format=\"JPEG\", quality=jpeg_quality)\n",
    "    compressed_size = img.size\n",
    "\n",
    "    return orig_size, compressed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55491f0-0ca2-4647-8e1d-4c5797f5c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(r\"D:\\images\\0a72b807-ecfc-3d55-98aa-f8a2f81b838f.jpg\")\n",
    "output_path = Path(r\"D:\\tmp.jpeg\")\n",
    "compress_image_on_disk(input_path, output_path, target_width=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff0e8f-57f0-4197-96d9-eb6626609be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_original_vs_compressed(original_path, compressed_path, zoom_box=(1000, 1000, 400, 400), output_path=None):\n",
    "    \"\"\"\n",
    "    Display original and compressed images side by side, with a zoomed region.\n",
    "    \n",
    "    Parameters:\n",
    "        original_path (str): Path to original image\n",
    "        compressed_path (str): Path to compressed image\n",
    "        zoom_box (tuple): (x, y, width, height) in ORIGINAL image coordinates\n",
    "    \"\"\"\n",
    "    from matplotlib.patches import Rectangle\n",
    "    # Load images\n",
    "    orig = Image.open(original_path)\n",
    "    comp = Image.open(compressed_path)\n",
    "\n",
    "    # Compute scaling factor for compressed image\n",
    "    scale_x = comp.width / orig.width\n",
    "    scale_y = comp.height / orig.height\n",
    "    zoom_box_scaled = (\n",
    "        int(zoom_box[0] * scale_x),\n",
    "        int(zoom_box[1] * scale_y),\n",
    "        int(zoom_box[2] * scale_x),\n",
    "        int(zoom_box[3] * scale_y)\n",
    "    )\n",
    "\n",
    "    # Side-by-side full images with rectangle\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axes[0].imshow(orig)\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[0].add_patch(Rectangle((zoom_box[0], zoom_box[1]), zoom_box[2], zoom_box[3],\n",
    "                                edgecolor='red', facecolor='none', lw=2))\n",
    "\n",
    "    axes[1].imshow(comp)\n",
    "    axes[1].set_title(\"Compressed\")\n",
    "    axes[1].axis(\"off\")\n",
    "    axes[1].add_patch(Rectangle((zoom_box_scaled[0], zoom_box_scaled[1]),\n",
    "                                zoom_box_scaled[2], zoom_box_scaled[3],\n",
    "                                edgecolor='red', facecolor='none', lw=2))\n",
    "\n",
    "    # Zoomed-in comparison\n",
    "    fig2, axes2 = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    x, y, w, h = zoom_box\n",
    "    axes2[0].imshow(orig.crop((x, y, x + w, y + h)))\n",
    "    axes2[0].set_title(\"Original zoom\")\n",
    "    axes2[0].axis(\"off\")\n",
    "\n",
    "    xs, ys, ws, hs = zoom_box_scaled\n",
    "    axes2[1].imshow(comp.crop((xs, ys, xs + ws, ys + hs)))\n",
    "    axes2[1].set_title(\"Compressed zoom\")\n",
    "    axes2[1].axis(\"off\")\n",
    "    if output_path is not None and isinstance(output_path, (Path, str)):\n",
    "        plt.savefig(output_path)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068acd1-a314-4e29-94b4-2530837eba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_original_vs_compressed(input_path, output_path, zoom_box=(1000,1000,100,100), output_path=Path(r\"D:\\resizing.jpeg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a1089d-7917-480f-9119-9577491c1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel resizing\n",
    "\n",
    "def resize_image_task(args):\n",
    "    path, output_dir, target_size = args\n",
    "    try:\n",
    "        filename = os.path.basename(path)\n",
    "        out_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            return f\"ERROR loading {path}\"\n",
    "\n",
    "        resized = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "        cv2.imwrite(out_path, resized)\n",
    "\n",
    "        return f\"OK   {filename}\"\n",
    "    except Exception as e:\n",
    "        return f\"ERROR {path}: {e}\"\n",
    "\n",
    "\n",
    "def resize_images(\n",
    "    input_dir,\n",
    "    output_dir,\n",
    "    target_size=(2048, 1500),\n",
    "    extensions=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".tif\")\n",
    "):\n",
    "    # Prepare file list\n",
    "    images = [\n",
    "        os.path.join(input_dir, f)\n",
    "        for f in os.listdir(input_dir)\n",
    "        if os.path.splitext(f)[1].lower() in extensions\n",
    "    ]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Found {len(images)} images…\")\n",
    "\n",
    "    # Prepare arguments for workers\n",
    "    tasks = [(img, output_dir, target_size) for img in images]\n",
    "\n",
    "    # Run multiprocessing pool\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        for result in pool.imap_unordered(resize_image_task, tasks):\n",
    "            print(result)\n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9686a-8338-44a1-93f0-692d25506e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path(r\"D:\\images\")\n",
    "output_dir = Path(r\"E:\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aebab4d-c867-4e8c-ae41-dba05b22f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    resize_images(\n",
    "        input_dir=input_dir,\n",
    "        output_dir=output_dir,\n",
    "        target_size=(2048, 1500)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
